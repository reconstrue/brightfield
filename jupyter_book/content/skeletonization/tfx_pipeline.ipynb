{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "tfx_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hOOUhbpGpgrr"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eo4WuL_pXCA",
        "colab_type": "text"
      },
      "source": [
        "# Brightfield Challenge as TFX Pipeline\n",
        "\n",
        "<img src=\"http://reconstrue.com/assets/images/reconstrue_combo_mark.svg\" width=\"266px\"/>\n",
        "\n",
        "This Jupyter notebook walks through the code for how to set up a TensorFlow Extended (TFX) pipeline as a development environment for researchers building neuron object recognition models trained on [the Allen Institute's Brightfield Challenge](http://reconstrue.com/data_sources/allen_institute/brightfield_neuron_reconstruction_challenge.html) dataset.\n",
        "\n",
        "TFX's InteractiveContext enables interactive construction of a pipeline that runs in the Jupyter kernel, making for a quick, lightweight training experience. Once the pipeline looks good, the same code can be exported and orchestrated on a scalable platform (Apache Airflow, Beam, Kubeflow). As such, TFX pipelines can unify the research/development and engineering/production parts of a model's lifecycles, all based on Jupyter notebooks.\n",
        "\n",
        "So, this notebook provides an fully functional TensorFlow development pipeline that researchers can adopt to jump past all the set up and data wrangling rigmarole and get down to the core work of model building. If the researchers build atop this notebook, then their code should more easily transition to a production environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOOUhbpGpgrr",
        "colab_type": "text"
      },
      "source": [
        "## Legal\n",
        "\n",
        "This file was forked from an official Tensorflow notebook, components.ipynb ([GitHub](https://github.com/tensorflow/tfx/blob/master/docs/tutorials/tfx/components.ipynb), [Colab](https://colab.research.google.com/github/tensorflow/tfx/blob/master/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_interactive.ipynb)).\n",
        "\n",
        "This code is licensed under the Apache License, Version 2.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4_SDIuvpTV3",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "# Copyright 2019-2020 Reconstrue LLC. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "# **Copyright &copy; 2019 The TensorFlow Authors.**\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf0YEDSENhQV",
        "colab_type": "text"
      },
      "source": [
        "## The Brightfield Challenge Dataset\n",
        "\n",
        "The Brightfield Challenge dataset consists of 115 neuron image stacks:\n",
        "- Training data: 105 neurons, with SWC files, totalling 2.2 TB\n",
        "- Test data: 10 neurons, without SWC files, totalling 261.3 GB\n",
        "\n",
        "The SWC files (tabular textual files of skeleton points in 3D) can be seen as the gold standard labeled data, what the system is supposed to output and can be evaluated against. \n",
        "\n",
        "The final goal of this system is to produce 10 SWC files, one for each of the test neuron image stacks. The largest single image stack is about 60 GB. A single, full neuron image stack is a logical unit of work, resulting in a single SWC file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vnMQAvTPrp7",
        "colab_type": "text"
      },
      "source": [
        "## TFX Primer\n",
        "\n",
        "In a Jupyter editor, TFX component pipelines can be used as an interactive yet reproducable developement environment for training, evalution, and tuning of models. The pipeline is non-scalably executed with the Jupyter kernel. Later for deployment, use the same code as interactively developed, simply change Context – same Pipeline APIs. So, code can smoothly transition from a Data Scientist experimental playground to production engineering.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_bpV_sQJKvc",
        "colab_type": "text"
      },
      "source": [
        "### Background\n",
        "\n",
        "Working in an interactive notebook is a useful way to become familiar with the structure of a TFX pipeline.  It's also useful when doing development of your own pipelines as a lightweight development environment, but you should be aware that there are differences in the way interactive notebooks are orchestrated, and how they access metadata artifacts.\n",
        "\n",
        "### Orchestration\n",
        "\n",
        "In a production deployment of TFX, you will use an orchestrator such as Apache Airflow, Kubeflow Pipelines, or Apache Beam to orchestrate a pre-defined pipeline graph of TFX components.  In an interactive notebook, the notebook itself is the orchestrator, running each TFX component as you execute the notebook cells.\n",
        "\n",
        "### Metadata\n",
        "\n",
        "In a production deployment of TFX, you will access metadata through the ML Metadata (MLMD) API.  MLMD stores metadata properties in a database such as MySQL or SQLite, and stores the metadata payloads in a persistent store such as on your filesystem.  In an interactive notebook, both properties and payloads are stored in an ephemeral SQLite database in the `/tmp` directory on the Jupyter notebook or Colab server."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYRTMoxSJamN",
        "colab_type": "text"
      },
      "source": [
        "### TFX Tech Talks\n",
        "\n",
        "Google has a YouTube playlist of [TFX videos](https://www.youtube.com/playlist?list=PLQY2H8rRoyvxR15n04JiW0ezF5HQRs_8F). The most relevant two are discussed further below.\n",
        "\n",
        "Others:\n",
        "- [TFX Overview and Pre-training Workflow](https://www.youtube.com/watch?v=A5wiwT1qFjc), TF Dev Summit '19,32 min, YouTube"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVk0N_dEdpIz",
        "colab_type": "text"
      },
      "source": [
        "#### TFX 2019-11 Talk\n",
        "**[TFX tech talk](https://youtu.be/TA5kbFgeUlk?t=1562)**, YouTube, 2019-11\n",
        "\n",
        "One of the presenter demoed [a Jupyter notebook on Colab](https://colab.research.google.com/github/tensorflow/tfx/blob/master/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_interactive.ipynb) demoing interactive dev via InteractiveContext:\n",
        "\n",
        "```python\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "```\n",
        "\n",
        "That file is what was used to seed this file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRDOrvnYHer1",
        "colab_type": "text"
      },
      "source": [
        "#### Pipelines on GCP \n",
        "\n",
        "At TensorFlow Dev Summit 2020 (2020-03) they beta'd Google Cloud AI Platform Pipelines (the Pipelines is the new part). [The tech press release](https://cloud.google.com/blog/products/ai-machine-learning/introducing-cloud-ai-platform-pipelines) is the best primer doc.\n",
        "\n",
        "Pipelines is TFX end to end pipelines running on Google Cloud AI Platform, as a new feature of the platform. Where TFX is platform agnostic, Pipelines it TFX deployed specifically atop Google Cloud AI Platform, e.g. KubeFlow happens to be the executor. That's provided as an easy-to-use fully managed service.\n",
        "\n",
        "Intro and demo talk, [TFX: Production ML with TensorFlow in 2020 (TF Dev Summit '20)](https://youtu.be/I3MjuFGmJrg?list=PLQY2H8rRoyvzuJw20FG82Lgm2SZjTdIXU). The PM intro pitch is only the first 4 minutes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2GivNBNYjb3b"
      },
      "source": [
        "## Setup\n",
        "First, we install and import the necessary packages, set up paths, and download data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN2sHZvuEl1O",
        "colab_type": "text"
      },
      "source": [
        "### Install AllenSDK\n",
        "\n",
        "The Challenge dataset resides in [the Allen Institute's Cell Types Database](http://reconstrue.com/data_sources/allen_institute/cell_types_db.html). To access their data resources, the Allen Institute provides a Python SDK, [the Allen SDK](http://reconstrue.com/data_sources/allen_institute/allensdk_on_colab.html). \n",
        "\n",
        "Colab comes with many modules pre-installed. The Allen SDK is not one of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XehG4UQ4Gh8Y",
        "colab_type": "code",
        "outputId": "8915765f-781d-4bba-84b9-cbab33acf593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install allensdk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting allensdk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/7a/163861e43ec4ac0b7d2e9ca2095a42fcecd914812751594d8c4338573a8f/allensdk-1.5.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.6/dist-packages (from allensdk) (2.21.0)\n",
            "Requirement already satisfied: future<1.0.0,>=0.14.3 in /usr/local/lib/python3.6/dist-packages (from allensdk) (0.16.0)\n",
            "Collecting tables==3.5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ab/79/4e1301a87f3b7f27aa6c9cb1aeba4875ff3edb62a6fe3872dc8f04983db4/tables-3.5.1-cp36-cp36m-manylinux1_x86_64.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 57.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib<4.0.0,>=1.4.3 in /usr/local/lib/python3.6/dist-packages (from allensdk) (3.2.0)\n",
            "Collecting statsmodels==0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d1/69ee7e757f657e7f527cbf500ec2d295396e5bcec873cf4eb68962c41024/statsmodels-0.9.0-cp36-cp36m-manylinux1_x86_64.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 31.5MB/s \n",
            "\u001b[?25hCollecting pynwb==1.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/72/c709ff36701cd4791cb3e296df7421acbac11699406c331eb9f10c79289b/pynwb-1.0.2-py2.py3-none-any.whl (96kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 14.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py<3.0.0,>=2.8 in /usr/local/lib/python3.6/dist-packages (from allensdk) (2.8.0)\n",
            "Collecting glymur==0.8.19\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/f9/01b464dd2b312c1f9bec26f985f2d0a9eef1a7390e407588578cc135fde0/Glymur-0.8.19.tar.gz (3.4MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 66.8MB/s \n",
            "\u001b[?25hCollecting simplejson<4.0.0,>=3.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/a7b98aa9256c8843f92878966dc3d8d914c14aad97e2c5ce4798d5743e07/simplejson-3.17.0.tar.gz (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.7MB/s \n",
            "\u001b[?25hCollecting scikit-build<1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/b5/c6ca60421991c22e69b9a950b0d046e06d714f79f7071946ab885c7115fb/scikit_build-0.10.0-py2.py3-none-any.whl (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.7MB/s \n",
            "\u001b[?25hCollecting requests-toolbelt<1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/ef/7681134338fc097acef8d9b2f8abe0458e4d87559c689a8c306d0957ece5/requests_toolbelt-0.9.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.19.0,>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from allensdk) (1.18.1)\n",
            "Collecting psycopg2-binary<3.0.0,>=2.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/c0/16303cef8d54fdcfae7be7880cf471f21449225687f61cc3be2a7ef4e6e5/psycopg2_binary-2.8.4-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 46.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2<2.12.0,>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from allensdk) (2.11.1)\n",
            "Requirement already satisfied: pandas<=0.25.3,>=0.25.1 in /usr/local/lib/python3.6/dist-packages (from allensdk) (0.25.3)\n",
            "Collecting marshmallow==3.0.0rc6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d6/b6b2b90a9e10edf99a8bea3bc5200619829f04c9d9c0c5b3839d68baf072/marshmallow-3.0.0rc6-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn<1.0.0 in /usr/local/lib/python3.6/dist-packages (from allensdk) (0.10.0)\n",
            "Requirement already satisfied: xarray<0.16.0 in /usr/local/lib/python3.6/dist-packages (from allensdk) (0.15.0)\n",
            "Requirement already satisfied: six<2.0.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from allensdk) (1.12.0)\n",
            "Collecting pynrrd<1.0.0,>=0.2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/92/00/ef17d52fd125f357d7ead95e823091b2344194d34ce94e2fe839184f48e7/pynrrd-0.4.2-py2.py3-none-any.whl\n",
            "Collecting nest-asyncio==1.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/81/f3/e59eb5fa5c41c7e6ae9741ed18534dbfae15ad29040a3927396678934b28/nest_asyncio-1.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: scikit-image<0.17.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from allensdk) (0.16.2)\n",
            "Collecting simpleitk<2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/d8/53338c34f71020725ffb3557846c80af96c29c03bc883551a2565aa68a7c/SimpleITK-1.2.4-cp36-cp36m-manylinux1_x86_64.whl (42.5MB)\n",
            "\u001b[K     |████████████████████████████████| 42.5MB 72kB/s \n",
            "\u001b[?25hCollecting argschema<2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f0/ec/bd27b60a9fcbae7c56e40d87c268a58b75c83181779e78235b320462dba5/argschema-1.17.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scipy<2.0.0,>=0.15.1 in /usr/local/lib/python3.6/dist-packages (from allensdk) (1.4.1)\n",
            "Collecting hdmf==1.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/a9/1f31bbee98035d7947da45e30530a5126a6207d6664c064314fc0d64155a/hdmf-1.0.2-py2.py3-none-any.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.8MB/s \n",
            "\u001b[?25hCollecting aiohttp==3.6.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/39/7eb5f98d24904e0f6d3edb505d4aa60e3ef83c0a58d6fe18244a51757247/aiohttp-3.6.2-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 51.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->allensdk) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->allensdk) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->allensdk) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0->allensdk) (2019.11.28)\n",
            "Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.6/dist-packages (from tables==3.5.1->allensdk) (2.7.1)\n",
            "Requirement already satisfied: mock>=2.0 in /usr/local/lib/python3.6/dist-packages (from tables==3.5.1->allensdk) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<4.0.0,>=1.4.3->allensdk) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<4.0.0,>=1.4.3->allensdk) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib<4.0.0,>=1.4.3->allensdk) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<4.0.0,>=1.4.3->allensdk) (1.1.0)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.6/dist-packages (from statsmodels==0.9.0->allensdk) (0.5.1)\n",
            "Collecting ruamel.yaml\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/92/59af3e38227b9cc14520bf1e59516d99ceca53e3b8448094248171e9432b/ruamel.yaml-0.16.10-py2.py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 57.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from glymur==0.8.19->allensdk) (45.2.0)\n",
            "Requirement already satisfied: wheel>=0.29.0 in /usr/local/lib/python3.6/dist-packages (from scikit-build<1.0.0->allensdk) (0.34.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from scikit-build<1.0.0->allensdk) (20.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2<2.12.0,>=2.7.3->allensdk) (1.1.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas<=0.25.3,>=0.25.1->allensdk) (2018.9)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.17.0,>=0.14.0->allensdk) (2.4)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.17.0,>=0.14.0->allensdk) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.17.0,>=0.14.0->allensdk) (1.1.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image<0.17.0,>=0.14.0->allensdk) (7.0.0)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/8f/0209fc5d975f839344c33c822ff2f7ef80f6b1e984673a5a68f960bfa583/yarl-1.4.2-cp36-cp36m-manylinux1_x86_64.whl (252kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 58.1MB/s \n",
            "\u001b[?25hCollecting idna-ssl>=1.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
            "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp==3.6.2->allensdk) (3.6.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp==3.6.2->allensdk) (19.3.0)\n",
            "Collecting multidict<5.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/2e/3ab2f1fb72571f75013db323a3799d505d99f3bc203513604f1ffb9b7858/multidict-4.7.5-cp36-cp36m-manylinux1_x86_64.whl (148kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 60.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock>=2.0->tables==3.5.1->allensdk) (5.4.4)\n",
            "Collecting ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.9\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/77/4bcd63f362bcb6c8f4f06253c11f9772f64189bf08cf3f40c5ccbda9e561/ruamel.yaml.clib-0.2.0-cp36-cp36m-manylinux1_x86_64.whl (548kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 51.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image<0.17.0,>=0.14.0->allensdk) (4.4.2)\n",
            "Building wheels for collected packages: glymur, simplejson, idna-ssl\n",
            "  Building wheel for glymur (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for glymur: filename=Glymur-0.8.19-cp36-none-any.whl size=2722000 sha256=e255221964e16ab228862632f00522b18fbd184c5954ef362e7b37b2ea6da4b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/c6/f6/918d148fb2aa6a13606af5475644e8116e9398485c36f0d995\n",
            "  Building wheel for simplejson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for simplejson: filename=simplejson-3.17.0-cp36-cp36m-linux_x86_64.whl size=114213 sha256=faf2eb721774c8beeba34587c2cf3c922568c68087ecae13511bc3da83963713\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/c0/83/dcd0339abb2640544bb8e0938aab2d069cef55e5647ce6e097\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3162 sha256=ae480c942f76051dabcc78e028864c52a92cdd5fe9a703e902325da0b37b780f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
            "Successfully built glymur simplejson idna-ssl\n",
            "Installing collected packages: tables, statsmodels, ruamel.yaml.clib, ruamel.yaml, hdmf, pynwb, glymur, simplejson, scikit-build, requests-toolbelt, psycopg2-binary, marshmallow, pynrrd, nest-asyncio, simpleitk, argschema, async-timeout, multidict, yarl, idna-ssl, aiohttp, allensdk\n",
            "  Found existing installation: tables 3.4.4\n",
            "    Uninstalling tables-3.4.4:\n",
            "      Successfully uninstalled tables-3.4.4\n",
            "  Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "Successfully installed aiohttp-3.6.2 allensdk-1.5.1 argschema-1.17.5 async-timeout-3.0.1 glymur-0.8.19 hdmf-1.0.2 idna-ssl-1.1.0 marshmallow-3.0.0rc6 multidict-4.7.5 nest-asyncio-1.2.0 psycopg2-binary-2.8.4 pynrrd-0.4.2 pynwb-1.0.2 requests-toolbelt-0.9.1 ruamel.yaml-0.16.10 ruamel.yaml.clib-0.2.0 scikit-build-0.10.0 simpleitk-1.2.4 simplejson-3.17.0 statsmodels-0.9.0 tables-3.5.1 yarl-1.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MZOYTt1RW4TK"
      },
      "source": [
        "### Install TFX\n",
        "\n",
        "Note: Because of package updates, you must use the button at the bottom of the output of this cell to restart the runtime. Following restart, please rerun this cell.\n",
        "\n",
        "(JFT: 2020-03-17: took < 2 minutes.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S4SQA7Q5nej3",
        "outputId": "95333025-9beb-4876-96a2-7e746695f95c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install \"tfx>=0.21.1,<0.22\" \"tensorflow>=2.1,<2.2\" \"tensorboard>=2.1,<2.2\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tfx<0.22,>=0.21.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/b0/360b8e4aa4a464645f1f9c866a668ac5f15bd65c4014cbf769b0aba649fc/tfx-0.21.2-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 8.7MB/s \n",
            "\u001b[?25hCollecting tensorflow<2.2,>=2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 41kB/s \n",
            "\u001b[?25hCollecting tensorboard<2.2,>=2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9MB 61.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<8,>=7 in /usr/local/lib/python3.6/dist-packages (from tfx<0.22,>=0.21.1) (7.1.1)\n",
            "Collecting absl-py<0.9,>=0.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/72/e6e483e2db953c11efa44ee21c5fdb6505c4dffa447b4263ca8af6676b62/absl-py-0.8.1.tar.gz (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 46.2MB/s \n",
            "\u001b[?25hCollecting ml-metadata<0.22,>=0.21.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/3d/a1717dd48ed7ee14670b51822f6bab14e73c3fabd641cb85354dd0d7471d/ml_metadata-0.21.2-cp36-cp36m-manylinux2010_x86_64.whl (4.9MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9MB 51.7MB/s \n",
            "\u001b[?25hCollecting tensorflow-model-analysis<0.22,>=0.21.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/a0/2e4eef99c8a308fa9e28fd2e9faa730e5e5f5b6c05c93ee8e2bb81075345/tensorflow_model_analysis-0.21.6-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 47.1MB/s \n",
            "\u001b[?25hCollecting tensorflow-serving-api<3,>=1.15\n",
            "  Downloading https://files.pythonhosted.org/packages/6d/0b/be364dc6271a633629174fc02d36b2837cc802250d6a0afd96f8e7f2fae6/tensorflow_serving_api-2.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: protobuf<4,>=3.7 in /usr/local/lib/python3.6/dist-packages (from tfx<0.22,>=0.21.1) (3.10.0)\n",
            "Collecting apache-beam[gcp]<2.18,>=2.17\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/80/b561617b7820c5607ed96e624e0b380dc613dcde70d5f39bc30c4345f5c0/apache_beam-2.17.0-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 60.3MB/s \n",
            "\u001b[?25hCollecting pyarrow<0.16,>=0.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/32/ce1926f05679ea5448fd3b98fbd9419d8c7a65f87d1a12ee5fb9577e3a8e/pyarrow-0.15.1-cp36-cp36m-manylinux2010_x86_64.whl (59.2MB)\n",
            "\u001b[K     |████████████████████████████████| 59.2MB 89kB/s \n",
            "\u001b[?25hCollecting tfx-bsl<0.22,>=0.21.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/76/3a6d9e0059ce615d0cdd3ca34a241e4b2ba21f5e6fbdc23283b31d98a0d0/tfx_bsl-0.21.4-cp36-cp36m-manylinux2010_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 61.7MB/s \n",
            "\u001b[?25hCollecting pyyaml<6,>=5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/d9/ea9816aea31beeadccd03f1f8b625ecf8f645bd66744484d162d84803ce5/PyYAML-5.3.tar.gz (268kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 53.1MB/s \n",
            "\u001b[?25hCollecting tensorflow-data-validation<0.22,>=0.21.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/d7/24a3932e633da767047974dbba2032991274e389cc10547d5ab0233939bc/tensorflow_data_validation-0.21.5-cp36-cp36m-manylinux2010_x86_64.whl (2.4MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4MB 51.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client<2,>=1.7.8 in /usr/local/lib/python3.6/dist-packages (from tfx<0.22,>=0.21.1) (1.7.11)\n",
            "Collecting docker<5,>=4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/58/74/379a9d30b1620def158c40b88c43e01c1936a287ebb97afab0699c601c57/docker-4.2.0-py2.py3-none-any.whl (143kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 70.7MB/s \n",
            "\u001b[?25hCollecting grpcio!=1.27.2,<2,>=1.25\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/6d/ee60b05e5e6df8d27561bcb2579b4b2e4004561be7c843768359e005093b/grpcio-1.27.1-cp36-cp36m-manylinux2010_x86_64.whl (2.7MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7MB 50.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from tfx<0.22,>=0.21.1) (1.12.0)\n",
            "Collecting tensorflow-transform<0.22,>=0.21.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/bd/8ba8c1310cd741e0b83d8a064645a55c557df5a2f6b4beb11cd3a37457ed/tensorflow-transform-0.21.2.tar.gz (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 57.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2<3,>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from tfx<0.22,>=0.21.1) (2.11.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1) (1.18.1)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1) (1.4.1)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 40.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1) (0.1.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1) (0.34.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1) (3.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1) (1.12.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.2,>=2.1) (0.2.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2,>=2.1) (1.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2,>=2.1) (3.2.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2,>=2.1) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2,>=2.1) (2.21.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2,>=2.1) (45.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2,>=2.1) (1.0.0)\n",
            "Requirement already satisfied: pandas<2,>=0.24 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (0.25.3)\n",
            "Requirement already satisfied: jupyter<2,>=1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (1.0.0)\n",
            "Requirement already satisfied: ipywidgets<8,>=7 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (7.5.1)\n",
            "Requirement already satisfied: tensorflow-metadata<0.22,>=0.21 in /usr/local/lib/python3.6/dist-packages (from tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (0.21.1)\n",
            "Collecting avro-python3<2.0.0,>=1.8.1; python_version >= \"3.0\"\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/80/acd1455bea0a9fcdc60a748a97dcbb3ff624726fb90987a0fc1c19e7a5a5/avro-python3-1.9.2.1.tar.gz\n",
            "Collecting hdfs<3.0.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/39/2c0879b1bcfd1f6ad078eb210d09dbce21072386a3997074ee91e60ddc5a/hdfs-2.5.8.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: future<1.0.0,>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<2.18,>=2.17->tfx<0.22,>=0.21.1) (0.16.0)\n",
            "Collecting mock<3.0.0,>=1.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.0MB/s \n",
            "\u001b[?25hCollecting dill<0.3.1,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/7a/70803635c850e351257029089d38748516a280864c97cbc73087afef6d51/dill-0.3.0.tar.gz (151kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 63.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<2.18,>=2.17->tfx<0.22,>=0.21.1) (2018.9)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<2.18,>=2.17->tfx<0.22,>=0.21.1) (1.3.0)\n",
            "Collecting fastavro<0.22,>=0.21.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/28/0206330c0002b1e28e21473117d0dc813defbd5891562d27af5c68c93899/fastavro-0.21.24-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 59.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<2.18,>=2.17->tfx<0.22,>=0.21.1) (3.10.1)\n",
            "Requirement already satisfied: httplib2<=0.12.0,>=0.8 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<2.18,>=2.17->tfx<0.22,>=0.21.1) (0.11.3)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<2.18,>=2.17->tfx<0.22,>=0.21.1) (1.7)\n",
            "Collecting oauth2client<4,>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/7b/bc893e35d6ca46a72faa4b9eaac25c687ce60e1fbe978993fe2de1b0ff0d/oauth2client-3.0.0.tar.gz (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<2.18,>=2.17->tfx<0.22,>=0.21.1) (2.8.1)\n",
            "Collecting google-cloud-bigtable<1.1.0,>=0.31.1; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/af/0ef7d097a1d5ad0c843867600e86de915e8ab8864740f49a4636cfb51af6/google_cloud_bigtable-1.0.0-py2.py3-none-any.whl (232kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 60.0MB/s \n",
            "\u001b[?25hCollecting google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/aa/29cbcf8cf7d08ce2d55b9dce858f7c632b434cb6451bed17cb4275804217/google_cloud_datastore-1.7.4-py2.py3-none-any.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 13.2MB/s \n",
            "\u001b[?25hCollecting google-cloud-bigquery<1.18.0,>=1.6.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/96/1b9cf1d43869c47a205aad411dac7c3040df6093d63c39273fa4d4c45da7/google_cloud_bigquery-1.17.1-py2.py3-none-any.whl (142kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 71.9MB/s \n",
            "\u001b[?25hCollecting google-cloud-pubsub<1.1.0,>=0.39.0; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/91/07a82945a7396ea34debafd476724bb5fc267c292790fdf2138c693f95c5/google_cloud_pubsub-1.0.2-py2.py3-none-any.whl (118kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 41.5MB/s \n",
            "\u001b[?25hCollecting google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/32/df3e36fd705a00092f1ffa9f41ce1df8dcb594ae313d239b87861a41fc2e/google-apitools-0.5.28.tar.gz (172kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 54.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-cloud-core<2,>=0.28.1; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<2.18,>=2.17->tfx<0.22,>=0.21.1) (1.0.3)\n",
            "Requirement already satisfied: cachetools<4,>=3.1.0; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]<2.18,>=2.17->tfx<0.22,>=0.21.1) (3.1.1)\n",
            "Requirement already satisfied: ipython>=5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-data-validation<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (5.5.0)\n",
            "Requirement already satisfied: joblib<0.15,>=0.12 in /usr/local/lib/python3.6/dist-packages (from tensorflow-data-validation<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (0.14.1)\n",
            "Collecting scikit-learn<0.22,>=0.18\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/c5/d2238762d780dde84a20b8c761f563fe882b88c5a5fb03c056547c442a19/scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 59.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client<2,>=1.7.8->tfx<0.22,>=0.21.1) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client<2,>=1.7.8->tfx<0.22,>=0.21.1) (0.0.3)\n",
            "Collecting websocket-client>=0.32.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 70.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2<3,>=2.7.3->tfx<0.22,>=0.21.1) (1.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow<2.2,>=2.1) (2.8.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2,>=2.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2,>=2.1) (4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2,>=2.1) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2,>=2.1) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2,>=2.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2,>=2.1) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2,>=2.1) (2.8)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (5.6.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (5.2.2)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (4.7.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (5.2.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (4.6.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (4.3.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (3.5.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (5.0.4)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata<0.22,>=0.21->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (1.51.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.6/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<2.18,>=2.17->tfx<0.22,>=0.21.1) (0.6.2)\n",
            "Collecting pbr>=0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/db/a968fd7beb9fe06901c1841cb25c9ccb666ca1b9a19b114d1bbedf1126fc/pbr-5.4.4-py2.py3-none-any.whl (110kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 57.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot<2,>=1.2.0->apache-beam[gcp]<2.18,>=2.17->tfx<0.22,>=0.21.1) (2.4.6)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]<2.18,>=2.17->tfx<0.22,>=0.21.1) (0.4.8)\n",
            "Collecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
            "  Downloading https://files.pythonhosted.org/packages/65/19/2060c8faa325fddc09aa67af98ffcb6813f39a0ad805679fa64815362b3a/grpc-google-iam-v1-0.12.3.tar.gz\n",
            "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigtable<1.1.0,>=0.31.1; extra == \"gcp\"->apache-beam[gcp]<2.18,>=2.17->tfx<0.22,>=0.21.1) (1.16.0)\n",
            "Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery<1.18.0,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]<2.18,>=2.17->tfx<0.22,>=0.21.1) (0.4.1)\n",
            "Collecting fasteners>=0.14\n",
            "  Downloading https://files.pythonhosted.org/packages/18/bd/55eb2d6397b9c0e263af9d091ebdb756b15756029b3cededf6461481bc63/fasteners-0.15-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5->tensorflow-data-validation<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=5->tensorflow-data-validation<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5->tensorflow-data-validation<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=5->tensorflow-data-validation<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=5->tensorflow-data-validation<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (2.1.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=5->tensorflow-data-validation<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (1.0.18)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2,>=2.1) (3.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (1.4.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (4.6.3)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (3.1.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (0.4.4)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (0.8.4)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (0.2.0)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (4.5.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (5.3.4)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter<2,>=1->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (0.8.3)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter<2,>=1->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (1.9.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (2.6.0)\n",
            "Collecting monotonic>=0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5->tensorflow-data-validation<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5->tensorflow-data-validation<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (0.1.8)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter<2,>=1->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (0.5.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->jupyter<2,>=1->tensorflow-model-analysis<0.22,>=0.21.4->tfx<0.22,>=0.21.1) (17.0.0)\n",
            "Building wheels for collected packages: absl-py, pyyaml, tensorflow-transform, avro-python3, hdfs, dill, oauth2client, google-apitools, grpc-google-iam-v1\n",
            "  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for absl-py: filename=absl_py-0.8.1-cp36-none-any.whl size=121167 sha256=c610b942ded6000893f9073fd8090e33a94feff7a3fda38b3ab2d409d65f1eb7\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/15/a0/0a0561549ad11cdc1bc8fa1191a353efd30facf6bfb507aefc\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3-cp36-cp36m-linux_x86_64.whl size=44229 sha256=2f8a14b5c8b1076b6941787e89d279e51b4a224cbe65b849efbb1985f2c2e37c\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/76/4d/a95b8dd7b452b69e8ed4f68b69e1b55e12c9c9624dd962b191\n",
            "  Building wheel for tensorflow-transform (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorflow-transform: filename=tensorflow_transform-0.21.2-cp36-none-any.whl size=301094 sha256=0d6dea941573b2cdd23418feb2c620b98034e952e32923e29b18ad35a16137c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/fa/f9/b167f10a3392a6d90659bb821c570255458f83ad5a4a321712\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-cp36-none-any.whl size=43516 sha256=83dc977ff2824bab6bad15f5ec6f1698f20103bcd55b4f4da1470b7224570e50\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/d3/be/86620c9dd3fca68986c33b9c616510289fc0abb81ec9aa70bd\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.5.8-cp36-none-any.whl size=33213 sha256=fb87a198e5bdd178ea977a0dc9bb37fc4cfca0d810f86b7da582b4229beda106\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/a7/05/23e3699975fc20f8a30e00ac1e515ab8c61168e982abe4ce70\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.0-cp36-none-any.whl size=77512 sha256=b192d15d2cc63cbcc4e827dde02b17dd2fd9f68bac1eaa583f2f721a53af1b16\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/de/a4/a91eec4eea652104d8c81b633f32ead5eb57d1b294eab24167\n",
            "  Building wheel for oauth2client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oauth2client: filename=oauth2client-3.0.0-cp36-none-any.whl size=106382 sha256=327170c49d9f5e939dd65b0e372e4b95d3eafc3ed6328628f21451f18138557c\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/f7/87/b932f09c6335dbcf45d916937105a372ab14f353a9ca431d7d\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.28-cp36-none-any.whl size=130111 sha256=1c1b5a0dee8d76655eca184652101c94de37b1dcdd50343e9c64776e16904710\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/c2/92/837e8a4d649a209dff85b38d7fbb576b4b480738be70865f29\n",
            "  Building wheel for grpc-google-iam-v1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grpc-google-iam-v1: filename=grpc_google_iam_v1-0.12.3-cp36-none-any.whl size=18500 sha256=6e789482e12653808036b996a5c8e4a4ab05f24a0847f5c4806d9c15d88f05b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/3a/83/77a1e18e1a8757186df834b86ce6800120ac9c79cd8ca4091b\n",
            "Successfully built absl-py pyyaml tensorflow-transform avro-python3 hdfs dill oauth2client google-apitools grpc-google-iam-v1\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement grpcio~=1.24.3, but you'll have grpcio 1.27.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement tensorflow-addons~=0.7.0, but you'll have tensorflow-addons 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pydrive 1.3.1 has requirement oauth2client>=4.0.0, but you'll have oauth2client 3.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: multiprocess 0.70.9 has requirement dill>=0.3.1, but you'll have dill 0.3.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: absl-py, grpcio, tensorflow-estimator, tensorboard, tensorflow, ml-metadata, pyarrow, avro-python3, hdfs, pbr, mock, dill, fastavro, oauth2client, grpc-google-iam-v1, google-cloud-bigtable, google-cloud-datastore, google-cloud-bigquery, google-cloud-pubsub, monotonic, fasteners, google-apitools, apache-beam, tensorflow-serving-api, tfx-bsl, tensorflow-model-analysis, pyyaml, tensorflow-transform, scikit-learn, tensorflow-data-validation, websocket-client, docker, tfx\n",
            "  Found existing installation: absl-py 0.9.0\n",
            "    Uninstalling absl-py-0.9.0:\n",
            "      Successfully uninstalled absl-py-0.9.0\n",
            "  Found existing installation: grpcio 1.24.3\n",
            "    Uninstalling grpcio-1.24.3:\n",
            "      Successfully uninstalled grpcio-1.24.3\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "  Found existing installation: dill 0.3.1.1\n",
            "    Uninstalling dill-0.3.1.1:\n",
            "      Successfully uninstalled dill-0.3.1.1\n",
            "  Found existing installation: oauth2client 4.1.3\n",
            "    Uninstalling oauth2client-4.1.3:\n",
            "      Successfully uninstalled oauth2client-4.1.3\n",
            "  Found existing installation: google-cloud-datastore 1.8.0\n",
            "    Uninstalling google-cloud-datastore-1.8.0:\n",
            "      Successfully uninstalled google-cloud-datastore-1.8.0\n",
            "  Found existing installation: google-cloud-bigquery 1.21.0\n",
            "    Uninstalling google-cloud-bigquery-1.21.0:\n",
            "      Successfully uninstalled google-cloud-bigquery-1.21.0\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed absl-py-0.8.1 apache-beam-2.17.0 avro-python3-1.9.2.1 dill-0.3.0 docker-4.2.0 fastavro-0.21.24 fasteners-0.15 google-apitools-0.5.28 google-cloud-bigquery-1.17.1 google-cloud-bigtable-1.0.0 google-cloud-datastore-1.7.4 google-cloud-pubsub-1.0.2 grpc-google-iam-v1-0.12.3 grpcio-1.27.1 hdfs-2.5.8 ml-metadata-0.21.2 mock-2.0.0 monotonic-1.5 oauth2client-3.0.0 pbr-5.4.4 pyarrow-0.15.1 pyyaml-5.3 scikit-learn-0.21.3 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-data-validation-0.21.5 tensorflow-estimator-2.1.0 tensorflow-model-analysis-0.21.6 tensorflow-serving-api-2.1.0 tensorflow-transform-0.21.2 tfx-0.21.2 tfx-bsl-0.21.4 websocket-client-0.57.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "grpc",
                  "pyarrow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA4MRJX875FK",
        "colab_type": "text"
      },
      "source": [
        "**Note:** [Author of notebook recommends restart VM at this point](https://youtu.be/TA5kbFgeUlk?t=1624) since the Python runtime may well have an internally cached earlier version of what was just installed. The above cell will also warn of the same:\n",
        "```\n",
        "WARNING: The following packages were previously imported in this runtime:\n",
        "  [google,grpc,pyarrow]\n",
        "You must restart the runtime in order to use newly installed versions.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N-ePgV0Lj68Q"
      },
      "source": [
        "### Import packages\n",
        "We import necessary packages, including standard TFX component classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YIqpWK9efviJ",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "import urllib\n",
        "\n",
        "import absl\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_analysis as tfma\n",
        "tf.get_logger().propagate = False\n",
        "pp = pprint.PrettyPrinter()\n",
        "\n",
        "import tfx\n",
        "from tfx.components import CsvExampleGen\n",
        "from tfx.components import Evaluator\n",
        "from tfx.components import ExampleValidator\n",
        "from tfx.components import Pusher\n",
        "from tfx.components import ResolverNode\n",
        "from tfx.components import SchemaGen\n",
        "from tfx.components import StatisticsGen\n",
        "from tfx.components import Trainer\n",
        "from tfx.components import Transform\n",
        "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
        "from tfx.orchestration import metadata\n",
        "from tfx.orchestration import pipeline\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "from tfx.proto import pusher_pb2\n",
        "from tfx.proto import trainer_pb2\n",
        "from tfx.types import Channel\n",
        "from tfx.types.standard_artifacts import Model\n",
        "from tfx.types.standard_artifacts import ModelBlessing\n",
        "from tfx.utils.dsl_utils import external_input\n",
        "\n",
        "%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wCZTHRy0N1D6"
      },
      "source": [
        "Let's check the library versions. We requested:\n",
        "- tfx>=0.21.1,<0.22\n",
        "- tensorflow>=2.1,<2.2\n",
        "- tensorboard>=2.1,<2.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eZ4K18_DN2D8",
        "colab": {}
      },
      "source": [
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "print('TFX version: {}'.format(tfx.__version__))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wumBHe66vB2m",
        "colab_type": "text"
      },
      "source": [
        "### Download Challenge dataset\n",
        "\n",
        "Initially the Challenge dataset resided in [Wasabi](https://wasabi.com/). Eventually, upon final production the dataset becomes simply part of the Allen Institute's database of thousands of cells. The code for acquiring the image and skeleton files needs to be adapted to each repository's coding interface."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBCZN6Li-x66",
        "colab_type": "text"
      },
      "source": [
        "#### Wasabi repository\n",
        "The full 115 neuron Challenge dataset is about 2.5 TB. A manifest of the files in the Challenge dataset, [specimens_manifest.json](http://reconstrue.com/projects/brightfield_neurons/challenge_dataset/specimens_manifest.json), is 3.1 MB. \n",
        "\n",
        "Note: specimens_manifest.json was generated during the original Challenge, at which time the files simply existed in an object bucket on Wasabi (Wasabi immitates the AWS S3 APIs and charges much less so it is a good choice for hosting terabytes of static files). \n",
        "\n",
        "The neurons in specimens_manifest.json are from the latest, greatest round of work by the Allen Institute (in an industrialized science pipeline that they are continually refining, so latest are greatest). As new neurons are fully processed they are included in the final repository, brain-map.org, which is where the Allen SDK pulls image files from. **At this time (2020-03) all those cell IDs are not in brain-map.org yet.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhdfWcRmAqSs",
        "colab_type": "code",
        "outputId": "a8c0568d-f48a-4e7e-80f2-8a6e54e9f298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "manifest_response = requests.get(manifest_url)\n",
        "print(f'status code: {manifest_response.status_code}')\n",
        "manifest_response.raise_for_status()\n",
        "manifest_as_text = manifest_response.text\n",
        "manifest = json.loads(manifest_response.text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "status code: 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jkRM7e3O7De",
        "colab_type": "code",
        "outputId": "fb8649d2-8b0b-4cd6-cecb-4800fa889407",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "# Dump out the head\n",
        "dump = json.dumps(manifest, sort_keys=True, indent=4, separators=(',', ': '))\n",
        "manifest_head = dump.splitlines()\n",
        "print(\"\\n\".join(manifest_head[0:20]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"647225829\": {\n",
            "        \"bucket_prefix\": \"647225829/\",\n",
            "        \"bytes\": 26559180540,\n",
            "        \"id\": \"647225829\",\n",
            "        \"image_stack\": [\n",
            "            \"647225829/reconstruction_0_0539044533_639893239-0001.tif\",\n",
            "            \"647225829/reconstruction_0_0539044533_639893239-0002.tif\",\n",
            "            \"647225829/reconstruction_0_0539044533_639893239-0003.tif\",\n",
            "            \"647225829/reconstruction_0_0539044533_639893239-0004.tif\",\n",
            "            \"647225829/reconstruction_0_0539044533_639893239-0005.tif\",\n",
            "            \"647225829/reconstruction_0_0539044533_639893239-0006.tif\",\n",
            "            \"647225829/reconstruction_0_0539044533_639893239-0007.tif\",\n",
            "            \"647225829/reconstruction_0_0539044533_639893239-0008.tif\",\n",
            "            \"647225829/reconstruction_0_0539044533_639893239-0009.tif\",\n",
            "            \"647225829/reconstruction_0_0539044533_639893239-0010.tif\",\n",
            "            \"647225829/reconstruction_0_0539044533_639893239-0011.tif\",\n",
            "            \"647225829/reconstruction_0_0539044533_639893239-0012.tif\",\n",
            "            \"647225829/reconstruction_0_0539044533_639893239-0013.tif\",\n",
            "            \"647225829/reconstruction_0_0539044533_639893239-0014.tif\",\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_pAJkO5i1EC",
        "colab_type": "text"
      },
      "source": [
        "TODO: Should cell id be int'd? Is currently string.\n",
        "```\n",
        "specimen_id: int\n",
        "ID of the specimen, from the Specimens database model in the Allen Institute API.            \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHcP3xihZU37",
        "colab_type": "code",
        "outputId": "1f7daea4-c0f0-4e2d-ec90-8b167b877aef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "challenge_cell_ids = [x for x in manifest] # dict to list\n",
        "print(f'# of cells: {len(challenge_cell_ids)}\\n') #114 is actually correct. One dup so not 115.\n",
        "\n",
        "print(challenge_cell_ids)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of cells: 114\n",
            "\n",
            "['647225829', '647244741', '647247980', '647278927', '647289876', '649052017', '650917845', '651511374', '651748297', '651790667', '651806289', '651829339', '651834134', '652113069', '654221379', '654591451', '663523681', '663961066', '664466860', '668664690', '669371214', '672278613', '673066511', '674317065', '676633030', '677326176', '677347027', '685884456', '687702530', '687746742', '688712523', '689485972', '691329423', '691830341', '692932326', '693441787', '693978543', '694569649', '694613686', '696228200', '696560235', '697851947', '699189400', '699207642', '702233284', '704338365', '704353262', '704363712', '706002308', '706065773', '707517873', '710114253', '710124691', '712951287', '712977942', '713016653', '713686035', '715273626', '715286106', '715328776', '718476684', '718706617', '718987297', '719458528', '720463180', '720948812', '721065710', '722033195', '722603466', '724316403', '726555942', '726635182', '728203498', '728251151', '729522604', '736979905', '739291676', '739383450', '741428906', '742421390', '743214898', '743274987', '743918700', '744609566', '745145893', '757721211', '762275581', '762912832', '765078615', '766985763', '767485082', '768977785', '772239618', '774495631', '777467421', '777472440', '797376860', '798631918', '815877776', '818150510', '821560343', '832210870', '836350796', '845142280', '861519869', '665856925', '687730329', '691311995', '715953708', '751017870', '761936495', '827413048', '850675694', '878858275']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tInmp1MztP3k",
        "colab_type": "text"
      },
      "source": [
        "#### brain-map.org repository\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHVA_JpxtPem",
        "colab_type": "code",
        "outputId": "15c73eb1-150a-44ab-d4f6-ed1999981e78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Query the Cell Types DB for cells having both skeletons and image stack.\n",
        "# Need both for traning context.\n",
        "\n",
        "# via https://allensdk.readthedocs.io/en/latest/cell_types.html#cell-types-cache\n",
        "from allensdk.core.cell_types_cache import CellTypesCache\n",
        "from allensdk.api.queries.cell_types_api import CellTypesApi\n",
        "\n",
        "ctc = CellTypesCache(manifest_file='cell_types/manifest.json')\n",
        "\n",
        "# We want mouse cells that have images and skeletons, both.\n",
        "# Former is data; latter is training labels a.k.a. gold standards.\n",
        "cells = ctc.get_cells(require_reconstruction=True, require_morphology=True, species=[CellTypesApi.MOUSE])\n",
        "print('Number of mouse cells with image stack and SWC files: %i' % len(cells))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of mouse cells with image stack and SWC files: 485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d8Px09CzLVI",
        "colab_type": "text"
      },
      "source": [
        "#### Temporary Dataset\n",
        "\n",
        "Since the cell ids are int which seem to be getting larger over time, sort the above set (mouse cells with images and skeletons) by cell id and take the more recent ones. Alternatively, could go by size of image stack rather than date (if want a small training set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JajDXFoiXNak",
        "colab_type": "text"
      },
      "source": [
        "# WIP: original doc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChhTy4_PXRZ-",
        "colab_type": "text"
      },
      "source": [
        "### Set up pipeline paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEGEOl-kXVpI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the root directory for your TFX pip package installation.\n",
        "_tfx_root = tfx.__path__[0]\n",
        "\n",
        "# This is the directory containing the TFX Chicago Taxi Pipeline example.\n",
        "_taxi_root = os.path.join(_tfx_root, 'examples/chicago_taxi_pipeline')\n",
        "\n",
        "# This is the path where your model will be pushed for serving.\n",
        "_serving_model_dir = os.path.join(\n",
        "    tempfile.mkdtemp(), 'serving_model/taxi_simple')\n",
        "\n",
        "# Set up logging.\n",
        "absl.logging.set_verbosity(absl.logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n2cMMAbSkGfX"
      },
      "source": [
        "### Download example data\n",
        "We download the example dataset for use in our TFX pipeline.\n",
        "\n",
        "The dataset we're using is the [Taxi Trips dataset](https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew) released by the City of Chicago. The columns in this dataset are:\n",
        "\n",
        "<table>\n",
        "<tr><td>pickup_community_area</td><td>fare</td><td>trip_start_month</td></tr>\n",
        "<tr><td>trip_start_hour</td><td>trip_start_day</td><td>trip_start_timestamp</td></tr>\n",
        "<tr><td>pickup_latitude</td><td>pickup_longitude</td><td>dropoff_latitude</td></tr>\n",
        "<tr><td>dropoff_longitude</td><td>trip_miles</td><td>pickup_census_tract</td></tr>\n",
        "<tr><td>dropoff_census_tract</td><td>payment_type</td><td>company</td></tr>\n",
        "<tr><td>trip_seconds</td><td>dropoff_community_area</td><td>tips</td></tr>\n",
        "</table>\n",
        "\n",
        "With this dataset, we will build a model that predicts the `tips` of a trip."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BywX6OUEhAqn",
        "colab": {}
      },
      "source": [
        "_data_root = tempfile.mkdtemp(prefix='tfx-data')\n",
        "DATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv'\n",
        "_data_filepath = os.path.join(_data_root, \"data.csv\")\n",
        "urllib.request.urlretrieve(DATA_PATH, _data_filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "blZC1sIQOWfH"
      },
      "source": [
        "Take a quick look at the CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c5YPeLPFOXaD",
        "colab": {}
      },
      "source": [
        "!head {_data_filepath}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8ONIE_hdkPS4"
      },
      "source": [
        "### Create the InteractiveContext\n",
        "Last, we create an InteractiveContext, which will allow us to run TFX components interactively in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Rh6K5sUf9dd",
        "colab": {}
      },
      "source": [
        "# Here, we create an InteractiveContext using default parameters. This will\n",
        "# use a temporary directory with an ephemeral ML Metadata database instance.\n",
        "# To use your own pipeline root or database, the optional properties\n",
        "# `pipeline_root` and `metadata_connection_config` may be passed to\n",
        "# InteractiveContext. Calls to InteractiveContext are no-ops outside of the\n",
        "# notebook.\n",
        "context = InteractiveContext()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HdQWxfsVkzdJ"
      },
      "source": [
        "## Run TFX components interactively\n",
        "In the cells that follow, we create TFX components one-by-one, run each of them, and visualize their output artifacts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L9fwt9gQk3BR"
      },
      "source": [
        "### ExampleGen\n",
        "\n",
        "The `ExampleGen` component is usually at the start of a TFX pipeline. It will:\n",
        "\n",
        "1.   Split data into training and evaluation sets (by default, 2/3 training + 1/3 eval)\n",
        "2.   Convert data into the `tf.Example` format\n",
        "3.   Copy data into the `_tfx_root` directory for other components to access\n",
        "\n",
        "`ExampleGen` takes as input the path to your data source. In our case, this is the `_data_root` path that contains the downloaded CSV.\n",
        "\n",
        "Note: In this notebook, we can instantiate components one-by-one and run them with `InteractiveContext.run()`. By contrast, in a production setting, we would specify all the components upfront in a `Pipeline` to pass to the orchestrator (see the \"Export to Pipeline\" section)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PyXjuMt8f-9u",
        "colab": {}
      },
      "source": [
        "example_gen = CsvExampleGen(input=external_input(_data_root))\n",
        "context.run(example_gen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OqCoZh7KPUm9"
      },
      "source": [
        "Let's examine the output artifacts of `ExampleGen`. This component produces two artifacts, training examples and evaluation examples:\n",
        "\n",
        "Note: The `%%skip_for_export` cell magic will omit the contents of this cell in the exported pipeline file (see the \"Export to pipeline\" section). This is useful for notebook-specific code that you don't want to run in an orchestrated pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "880KkTAkPeUg",
        "colab": {}
      },
      "source": [
        "%%skip_for_export\n",
        "\n",
        "artifact = example_gen.outputs['examples'].get()[0]\n",
        "print(artifact.split_names, artifact.uri)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J6vcbW_wPqvl"
      },
      "source": [
        "We can also take a look at the first three training examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H4XIXjiCPwzQ",
        "colab": {}
      },
      "source": [
        "%%skip_for_export\n",
        "\n",
        "# Get the URI of the output artifact representing the training examples, which is a directory\n",
        "train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'train')\n",
        "\n",
        "# Get the list of files in this directory (all compressed TFRecord files)\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                      for name in os.listdir(train_uri)]\n",
        "\n",
        "# Create a `TFRecordDataset` to read these files\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "\n",
        "# Iterate over the first 3 records and decode them.\n",
        "for tfrecord in dataset.take(3):\n",
        "  serialized_example = tfrecord.numpy()\n",
        "  example = tf.train.Example()\n",
        "  example.ParseFromString(serialized_example)\n",
        "  pp.pprint(example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2gluYjccf-IP"
      },
      "source": [
        "Now that `ExampleGen` has finished ingesting the data, the next step is data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "csM6BFhtk5Aa"
      },
      "source": [
        "### StatisticsGen\n",
        "The `StatisticsGen` component computes statistics over your dataset for data analysis, as well as for use in downstream components. It uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library.\n",
        "\n",
        "`StatisticsGen` takes as input the dataset we just ingested using `ExampleGen`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MAscCCYWgA-9",
        "colab": {}
      },
      "source": [
        "statistics_gen = StatisticsGen(\n",
        "    examples=example_gen.outputs['examples'])\n",
        "context.run(statistics_gen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HLI6cb_5WugZ"
      },
      "source": [
        "After `StatisticsGen` finishes running, we can visualize the outputted statistics. Try playing with the different plots!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tLjXy7K6Tp_G",
        "colab": {}
      },
      "source": [
        "%%skip_for_export\n",
        "\n",
        "context.show(statistics_gen.outputs['statistics'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HLKLTO9Nk60p"
      },
      "source": [
        "### SchemaGen\n",
        "\n",
        "The `SchemaGen` component generates a schema based on your data statistics. (A schema defines the expected bounds, types, and properties of the features in your dataset.) It also uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library.\n",
        "\n",
        "`SchemaGen` will take as input the statistics that we generated with `StatisticsGen`, looking at the training split by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ygQvZ6hsiQ_J",
        "colab": {}
      },
      "source": [
        "schema_gen = SchemaGen(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    infer_feature_shape=False)\n",
        "context.run(schema_gen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zi6TxTUKXM6b"
      },
      "source": [
        "After `SchemaGen` finishes running, we can visualize the generated schema as a table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ec9vqDXpXeMb",
        "colab": {}
      },
      "source": [
        "%%skip_for_export\n",
        "\n",
        "context.show(schema_gen.outputs['schema'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kZWWdbA-m7zp"
      },
      "source": [
        "Each feature in your dataset shows up as a row in the schema table, alongside its properties. The schema also captures all the values that a categorical feature takes on, denoted as its domain.\n",
        "\n",
        "To learn more about schemas, see [the SchemaGen documentation](https://www.tensorflow.org/tfx/guide/schemagen)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V1qcUuO9k9f8"
      },
      "source": [
        "### ExampleValidator\n",
        "The `ExampleValidator` component detects anomalies in your data, based on the expectations defined by the schema. It also uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library.\n",
        "\n",
        "`ExampleValidator` will take as input the statistics from `StatisticsGen`, and the schema from `SchemaGen`.\n",
        "\n",
        "By default, it compares the statistics from the evaluation split to the schema from the training split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XRlRUuGgiXks",
        "colab": {}
      },
      "source": [
        "example_validator = ExampleValidator(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    schema=schema_gen.outputs['schema'])\n",
        "context.run(example_validator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "855mrHgJcoer"
      },
      "source": [
        "After `ExampleValidator` finishes running, we can visualize the anomalies as a table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TDyAAozQcrk3",
        "colab": {}
      },
      "source": [
        "%%skip_for_export\n",
        "\n",
        "context.show(example_validator.outputs['anomalies'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "znMoJj60ybZx"
      },
      "source": [
        "In the anomalies table, we can see that the `company` feature takes on new values that were not in the training split. This information can be used to debug model performance, understand how your data evolves over time, and identify data errors.\n",
        "\n",
        "In our case, this anomaly is innocuous, so we move on to the next step of transforming the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JPViEz5RlA36"
      },
      "source": [
        "### Transform\n",
        "The `Transform` component performs feature engineering for both training and serving. It uses the [TensorFlow Transform](https://www.tensorflow.org/tfx/transform/get_started) library.\n",
        "\n",
        "`Transform` will take as input the data from `ExampleGen`, the schema from `SchemaGen`, as well as a module that contains user-defined Transform code.\n",
        "\n",
        "Let's see an example of user-defined Transform code below (for an introduction to the TensorFlow Transform APIs, [see the tutorial](https://www.tensorflow.org/tfx/tutorials/transform/simple)). First, we define a few constants for feature engineering:\n",
        "\n",
        "Note: The `%%writefile` cell magic will save the contents of the cell as a `.py` file on disk. This allows the `Transform` component to load your code as a module.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PuNSiUKb4YJf",
        "colab": {}
      },
      "source": [
        "_taxi_constants_module_file = 'taxi_constants.py'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HPjhXuIF4YJh",
        "colab": {}
      },
      "source": [
        "%%skip_for_export\n",
        "%%writefile {_taxi_constants_module_file}\n",
        "\n",
        "# Categorical features are assumed to each have a maximum value in the dataset.\n",
        "MAX_CATEGORICAL_FEATURE_VALUES = [24, 31, 12]\n",
        "\n",
        "CATEGORICAL_FEATURE_KEYS = [\n",
        "    'trip_start_hour', 'trip_start_day', 'trip_start_month',\n",
        "    'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area',\n",
        "    'dropoff_community_area'\n",
        "]\n",
        "\n",
        "DENSE_FLOAT_FEATURE_KEYS = ['trip_miles', 'fare', 'trip_seconds']\n",
        "\n",
        "# Number of buckets used by tf.transform for encoding each feature.\n",
        "FEATURE_BUCKET_COUNT = 10\n",
        "\n",
        "BUCKET_FEATURE_KEYS = [\n",
        "    'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n",
        "    'dropoff_longitude'\n",
        "]\n",
        "\n",
        "# Number of vocabulary terms used for encoding VOCAB_FEATURES by tf.transform\n",
        "VOCAB_SIZE = 1000\n",
        "\n",
        "# Count of out-of-vocab buckets in which unrecognized VOCAB_FEATURES are hashed.\n",
        "OOV_SIZE = 10\n",
        "\n",
        "VOCAB_FEATURE_KEYS = [\n",
        "    'payment_type',\n",
        "    'company',\n",
        "]\n",
        "\n",
        "# Keys\n",
        "LABEL_KEY = 'tips'\n",
        "FARE_KEY = 'fare'\n",
        "\n",
        "def transformed_name(key):\n",
        "  return key + '_xf'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Duj2Ax5z4YJl"
      },
      "source": [
        "Next, we write a `preprocessing_fn` that takes in raw data as input, and returns transformed features that our model can train on:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4AJ9hBs94YJm",
        "colab": {}
      },
      "source": [
        "_taxi_transform_module_file = 'taxi_transform.py'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MYmxxx9A4YJn",
        "colab": {}
      },
      "source": [
        "%%skip_for_export\n",
        "%%writefile {_taxi_transform_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "import taxi_constants\n",
        "\n",
        "_DENSE_FLOAT_FEATURE_KEYS = taxi_constants.DENSE_FLOAT_FEATURE_KEYS\n",
        "_VOCAB_FEATURE_KEYS = taxi_constants.VOCAB_FEATURE_KEYS\n",
        "_VOCAB_SIZE = taxi_constants.VOCAB_SIZE\n",
        "_OOV_SIZE = taxi_constants.OOV_SIZE\n",
        "_FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT\n",
        "_BUCKET_FEATURE_KEYS = taxi_constants.BUCKET_FEATURE_KEYS\n",
        "_CATEGORICAL_FEATURE_KEYS = taxi_constants.CATEGORICAL_FEATURE_KEYS\n",
        "_FARE_KEY = taxi_constants.FARE_KEY\n",
        "_LABEL_KEY = taxi_constants.LABEL_KEY\n",
        "_transformed_name = taxi_constants.transformed_name\n",
        "\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "  \"\"\"tf.transform's callback function for preprocessing inputs.\n",
        "  Args:\n",
        "    inputs: map from feature keys to raw not-yet-transformed features.\n",
        "  Returns:\n",
        "    Map from string feature key to transformed feature operations.\n",
        "  \"\"\"\n",
        "  outputs = {}\n",
        "  for key in _DENSE_FLOAT_FEATURE_KEYS:\n",
        "    # Preserve this feature as a dense float, setting nan's to the mean.\n",
        "    outputs[_transformed_name(key)] = tft.scale_to_z_score(\n",
        "        _fill_in_missing(inputs[key]))\n",
        "\n",
        "  for key in _VOCAB_FEATURE_KEYS:\n",
        "    # Build a vocabulary for this feature.\n",
        "    outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(\n",
        "        _fill_in_missing(inputs[key]),\n",
        "        top_k=_VOCAB_SIZE,\n",
        "        num_oov_buckets=_OOV_SIZE)\n",
        "\n",
        "  for key in _BUCKET_FEATURE_KEYS:\n",
        "    outputs[_transformed_name(key)] = tft.bucketize(\n",
        "        _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT,\n",
        "        always_return_num_quantiles=False)\n",
        "\n",
        "  for key in _CATEGORICAL_FEATURE_KEYS:\n",
        "    outputs[_transformed_name(key)] = _fill_in_missing(inputs[key])\n",
        "\n",
        "  # Was this passenger a big tipper?\n",
        "  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])\n",
        "  tips = _fill_in_missing(inputs[_LABEL_KEY])\n",
        "  outputs[_transformed_name(_LABEL_KEY)] = tf.where(\n",
        "      tf.math.is_nan(taxi_fare),\n",
        "      tf.cast(tf.zeros_like(taxi_fare), tf.int64),\n",
        "      # Test if the tip was > 20% of the fare.\n",
        "      tf.cast(\n",
        "          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))\n",
        "\n",
        "  return outputs\n",
        "\n",
        "\n",
        "def _fill_in_missing(x):\n",
        "  \"\"\"Replace missing values in a SparseTensor.\n",
        "  Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n",
        "  Args:\n",
        "    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n",
        "      in the second dimension.\n",
        "  Returns:\n",
        "    A rank 1 tensor where missing values of `x` have been filled in.\n",
        "  \"\"\"\n",
        "  default_value = '' if x.dtype == tf.string else 0\n",
        "  return tf.squeeze(\n",
        "      tf.sparse.to_dense(\n",
        "          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
        "          default_value),\n",
        "      axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wgbmZr3sgbWW"
      },
      "source": [
        "Now, we pass in this feature engineering code to the `Transform` component and run it to transform your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jHfhth_GiZI9",
        "colab": {}
      },
      "source": [
        "transform = Transform(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    module_file=os.path.abspath(_taxi_transform_module_file))\n",
        "context.run(transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fwAwb4rARRQ2"
      },
      "source": [
        "Let's examine the output artifacts of `Transform`. This component produces two types of outputs:\n",
        "\n",
        "* `transform_graph` is the graph that can perform the preprocessing operations (this graph will be included in the serving and evaluation models).\n",
        "* `transformed_examples` represents the preprocessed training and evaluation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SClrAaEGR1O5",
        "colab": {}
      },
      "source": [
        "transform.outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vyFkBd9AR1sy"
      },
      "source": [
        "Take a peek at the `transform_graph` artifact.  It points to a directory containing three subdirectories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5tRw4DneR3i7",
        "colab": {}
      },
      "source": [
        "train_uri = transform.outputs['transform_graph'].get()[0].uri\n",
        "os.listdir(train_uri)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4fqV54CIR6Pu"
      },
      "source": [
        "The `transformed_metadata` subdirectory contains the schema of the preprocessed data. The `transform_fn` subdirectory contains the actual preprocessing graph. The `metadata` subdirectory contains the schema of the original data.\n",
        "\n",
        "We can also take a look at the first three transformed examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pwbW2zPKR_S4",
        "colab": {}
      },
      "source": [
        "# Get the URI of the output artifact representing the transformed examples, which is a directory\n",
        "train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'train')\n",
        "\n",
        "# Get the list of files in this directory (all compressed TFRecord files)\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                      for name in os.listdir(train_uri)]\n",
        "\n",
        "# Create a `TFRecordDataset` to read these files\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "\n",
        "# Iterate over the first 3 records and decode them.\n",
        "for tfrecord in dataset.take(3):\n",
        "  serialized_example = tfrecord.numpy()\n",
        "  example = tf.train.Example()\n",
        "  example.ParseFromString(serialized_example)\n",
        "  pp.pprint(example)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q_b_V6eN4f69"
      },
      "source": [
        "After the `Transform` component has transformed your data into features, and the next step is to train a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OBJFtnl6lCg9"
      },
      "source": [
        "### Trainer\n",
        "The `Trainer` component will train a model that you define in TensorFlow (either using the Estimator API or the Keras API with [`model_to_estimator`](https://www.tensorflow.org/api_docs/python/tf/keras/estimator/model_to_estimator)).\n",
        "\n",
        "`Trainer` takes as input the schema from `SchemaGen`, the transformed data and graph from `Transform`, training parameters, as well as a module that contains user-defined model code.\n",
        "\n",
        "Let's see an example of user-defined model code below (for an introduction to the TensorFlow Estimator APIs, [see the tutorial](https://www.tensorflow.org/tutorials/estimator/premade)):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N1376oq04YJt",
        "colab": {}
      },
      "source": [
        "_taxi_trainer_module_file = 'taxi_trainer.py'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nf9UuNng4YJu",
        "colab": {}
      },
      "source": [
        "%%skip_for_export\n",
        "%%writefile {_taxi_trainer_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_analysis as tfma\n",
        "import tensorflow_transform as tft\n",
        "from tensorflow_transform.tf_metadata import schema_utils\n",
        "\n",
        "import taxi_constants\n",
        "\n",
        "_DENSE_FLOAT_FEATURE_KEYS = taxi_constants.DENSE_FLOAT_FEATURE_KEYS\n",
        "_VOCAB_FEATURE_KEYS = taxi_constants.VOCAB_FEATURE_KEYS\n",
        "_VOCAB_SIZE = taxi_constants.VOCAB_SIZE\n",
        "_OOV_SIZE = taxi_constants.OOV_SIZE\n",
        "_FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT\n",
        "_BUCKET_FEATURE_KEYS = taxi_constants.BUCKET_FEATURE_KEYS\n",
        "_CATEGORICAL_FEATURE_KEYS = taxi_constants.CATEGORICAL_FEATURE_KEYS\n",
        "_MAX_CATEGORICAL_FEATURE_VALUES = taxi_constants.MAX_CATEGORICAL_FEATURE_VALUES\n",
        "_LABEL_KEY = taxi_constants.LABEL_KEY\n",
        "_transformed_name = taxi_constants.transformed_name\n",
        "\n",
        "\n",
        "def _transformed_names(keys):\n",
        "  return [_transformed_name(key) for key in keys]\n",
        "\n",
        "\n",
        "# Tf.Transform considers these features as \"raw\"\n",
        "def _get_raw_feature_spec(schema):\n",
        "  return schema_utils.schema_as_feature_spec(schema).feature_spec\n",
        "\n",
        "\n",
        "def _gzip_reader_fn(filenames):\n",
        "  \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"\n",
        "  return tf.data.TFRecordDataset(\n",
        "      filenames,\n",
        "      compression_type='GZIP')\n",
        "\n",
        "\n",
        "def _build_estimator(config, hidden_units=None, warm_start_from=None):\n",
        "  \"\"\"Build an estimator for predicting the tipping behavior of taxi riders.\n",
        "  Args:\n",
        "    config: tf.estimator.RunConfig defining the runtime environment for the\n",
        "      estimator (including model_dir).\n",
        "    hidden_units: [int], the layer sizes of the DNN (input layer first)\n",
        "    warm_start_from: Optional directory to warm start from.\n",
        "  Returns:\n",
        "    A dict of the following:\n",
        "      - estimator: The estimator that will be used for training and eval.\n",
        "      - train_spec: Spec for training.\n",
        "      - eval_spec: Spec for eval.\n",
        "      - eval_input_receiver_fn: Input function for eval.\n",
        "  \"\"\"\n",
        "  real_valued_columns = [\n",
        "      tf.feature_column.numeric_column(key, shape=())\n",
        "      for key in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)\n",
        "  ]\n",
        "  categorical_columns = [\n",
        "      tf.feature_column.categorical_column_with_identity(\n",
        "          key, num_buckets=_VOCAB_SIZE + _OOV_SIZE, default_value=0)\n",
        "      for key in _transformed_names(_VOCAB_FEATURE_KEYS)\n",
        "  ]\n",
        "  categorical_columns += [\n",
        "      tf.feature_column.categorical_column_with_identity(\n",
        "          key, num_buckets=_FEATURE_BUCKET_COUNT, default_value=0)\n",
        "      for key in _transformed_names(_BUCKET_FEATURE_KEYS)\n",
        "  ]\n",
        "  categorical_columns += [\n",
        "      tf.feature_column.categorical_column_with_identity(  # pylint: disable=g-complex-comprehension\n",
        "          key,\n",
        "          num_buckets=num_buckets,\n",
        "          default_value=0) for key, num_buckets in zip(\n",
        "              _transformed_names(_CATEGORICAL_FEATURE_KEYS),\n",
        "              _MAX_CATEGORICAL_FEATURE_VALUES)\n",
        "  ]\n",
        "  return tf.estimator.DNNLinearCombinedClassifier(\n",
        "      config=config,\n",
        "      linear_feature_columns=categorical_columns,\n",
        "      dnn_feature_columns=real_valued_columns,\n",
        "      dnn_hidden_units=hidden_units or [100, 70, 50, 25],\n",
        "      warm_start_from=warm_start_from)\n",
        "\n",
        "\n",
        "def _example_serving_receiver_fn(tf_transform_graph, schema):\n",
        "  \"\"\"Build the serving in inputs.\n",
        "  Args:\n",
        "    tf_transform_graph: A TFTransformOutput.\n",
        "    schema: the schema of the input data.\n",
        "  Returns:\n",
        "    Tensorflow graph which parses examples, applying tf-transform to them.\n",
        "  \"\"\"\n",
        "  raw_feature_spec = _get_raw_feature_spec(schema)\n",
        "  raw_feature_spec.pop(_LABEL_KEY)\n",
        "\n",
        "  raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
        "      raw_feature_spec, default_batch_size=None)\n",
        "  serving_input_receiver = raw_input_fn()\n",
        "\n",
        "  transformed_features = tf_transform_graph.transform_raw_features(\n",
        "      serving_input_receiver.features)\n",
        "\n",
        "  return tf.estimator.export.ServingInputReceiver(\n",
        "      transformed_features, serving_input_receiver.receiver_tensors)\n",
        "\n",
        "\n",
        "def _eval_input_receiver_fn(tf_transform_graph, schema):\n",
        "  \"\"\"Build everything needed for the tf-model-analysis to run the model.\n",
        "  Args:\n",
        "    tf_transform_graph: A TFTransformOutput.\n",
        "    schema: the schema of the input data.\n",
        "  Returns:\n",
        "    EvalInputReceiver function, which contains:\n",
        "      - Tensorflow graph which parses raw untransformed features, applies the\n",
        "        tf-transform preprocessing operators.\n",
        "      - Set of raw, untransformed features.\n",
        "      - Label against which predictions will be compared.\n",
        "  \"\"\"\n",
        "  # Notice that the inputs are raw features, not transformed features here.\n",
        "  raw_feature_spec = _get_raw_feature_spec(schema)\n",
        "\n",
        "  serialized_tf_example = tf.compat.v1.placeholder(\n",
        "      dtype=tf.string, shape=[None], name='input_example_tensor')\n",
        "\n",
        "  # Add a parse_example operator to the tensorflow graph, which will parse\n",
        "  # raw, untransformed, tf examples.\n",
        "  features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n",
        "\n",
        "  # Now that we have our raw examples, process them through the tf-transform\n",
        "  # function computed during the preprocessing step.\n",
        "  transformed_features = tf_transform_graph.transform_raw_features(\n",
        "      features)\n",
        "\n",
        "  # The key name MUST be 'examples'.\n",
        "  receiver_tensors = {'examples': serialized_tf_example}\n",
        "\n",
        "  # NOTE: Model is driven by transformed features (since training works on the\n",
        "  # materialized output of TFT, but slicing will happen on raw features.\n",
        "  features.update(transformed_features)\n",
        "\n",
        "  return tfma.export.EvalInputReceiver(\n",
        "      features=features,\n",
        "      receiver_tensors=receiver_tensors,\n",
        "      labels=transformed_features[_transformed_name(_LABEL_KEY)])\n",
        "\n",
        "\n",
        "def _input_fn(filenames, tf_transform_graph, batch_size=200):\n",
        "  \"\"\"Generates features and labels for training or evaluation.\n",
        "  Args:\n",
        "    filenames: [str] list of CSV files to read data from.\n",
        "    tf_transform_graph: A TFTransformOutput.\n",
        "    batch_size: int First dimension size of the Tensors returned by input_fn\n",
        "  Returns:\n",
        "    A (features, indices) tuple where features is a dictionary of\n",
        "      Tensors, and indices is a single Tensor of label indices.\n",
        "  \"\"\"\n",
        "  transformed_feature_spec = (\n",
        "      tf_transform_graph.transformed_feature_spec().copy())\n",
        "\n",
        "  dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "      filenames, batch_size, transformed_feature_spec, reader=_gzip_reader_fn)\n",
        "\n",
        "  transformed_features = (\n",
        "      tf.compat.v1.data.make_one_shot_iterator(dataset).get_next())\n",
        "  # We pop the label because we do not want to use it as a feature while we're\n",
        "  # training.\n",
        "  return transformed_features, transformed_features.pop(\n",
        "      _transformed_name(_LABEL_KEY))\n",
        "\n",
        "\n",
        "# TFX will call this function\n",
        "def trainer_fn(trainer_fn_args, schema):\n",
        "  \"\"\"Build the estimator using the high level API.\n",
        "  Args:\n",
        "    trainer_fn_args: Holds args used to train the model as name/value pairs.\n",
        "    schema: Holds the schema of the training examples.\n",
        "  Returns:\n",
        "    A dict of the following:\n",
        "      - estimator: The estimator that will be used for training and eval.\n",
        "      - train_spec: Spec for training.\n",
        "      - eval_spec: Spec for eval.\n",
        "      - eval_input_receiver_fn: Input function for eval.\n",
        "  \"\"\"\n",
        "  # Number of nodes in the first layer of the DNN\n",
        "  first_dnn_layer_size = 100\n",
        "  num_dnn_layers = 4\n",
        "  dnn_decay_factor = 0.7\n",
        "\n",
        "  train_batch_size = 40\n",
        "  eval_batch_size = 40\n",
        "\n",
        "  tf_transform_graph = tft.TFTransformOutput(trainer_fn_args.transform_output)\n",
        "\n",
        "  train_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda\n",
        "      trainer_fn_args.train_files,\n",
        "      tf_transform_graph,\n",
        "      batch_size=train_batch_size)\n",
        "\n",
        "  eval_input_fn = lambda: _input_fn(  # pylint: disable=g-long-lambda\n",
        "      trainer_fn_args.eval_files,\n",
        "      tf_transform_graph,\n",
        "      batch_size=eval_batch_size)\n",
        "\n",
        "  train_spec = tf.estimator.TrainSpec(  # pylint: disable=g-long-lambda\n",
        "      train_input_fn,\n",
        "      max_steps=trainer_fn_args.train_steps)\n",
        "\n",
        "  serving_receiver_fn = lambda: _example_serving_receiver_fn(  # pylint: disable=g-long-lambda\n",
        "      tf_transform_graph, schema)\n",
        "\n",
        "  exporter = tf.estimator.FinalExporter('chicago-taxi', serving_receiver_fn)\n",
        "  eval_spec = tf.estimator.EvalSpec(\n",
        "      eval_input_fn,\n",
        "      steps=trainer_fn_args.eval_steps,\n",
        "      exporters=[exporter],\n",
        "      name='chicago-taxi-eval')\n",
        "\n",
        "  run_config = tf.estimator.RunConfig(\n",
        "      save_checkpoints_steps=999, keep_checkpoint_max=1)\n",
        "\n",
        "  run_config = run_config.replace(model_dir=trainer_fn_args.serving_model_dir)\n",
        "\n",
        "  estimator = _build_estimator(\n",
        "      # Construct layers sizes with exponetial decay\n",
        "      hidden_units=[\n",
        "          max(2, int(first_dnn_layer_size * dnn_decay_factor**i))\n",
        "          for i in range(num_dnn_layers)\n",
        "      ],\n",
        "      config=run_config,\n",
        "      warm_start_from=trainer_fn_args.base_model)\n",
        "\n",
        "  # Create an input receiver for TFMA processing\n",
        "  receiver_fn = lambda: _eval_input_receiver_fn(  # pylint: disable=g-long-lambda\n",
        "      tf_transform_graph, schema)\n",
        "\n",
        "  return {\n",
        "      'estimator': estimator,\n",
        "      'train_spec': train_spec,\n",
        "      'eval_spec': eval_spec,\n",
        "      'eval_input_receiver_fn': receiver_fn\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GY4yTRaX4YJx"
      },
      "source": [
        "Now, we pass in this model code to the `Trainer` component and run it to train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "429-vvCWibO0",
        "colab": {}
      },
      "source": [
        "trainer = Trainer(\n",
        "    module_file=os.path.abspath(_taxi_trainer_module_file),\n",
        "    transformed_examples=transform.outputs['transformed_examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    transform_graph=transform.outputs['transform_graph'],\n",
        "    train_args=trainer_pb2.TrainArgs(num_steps=10000),\n",
        "    eval_args=trainer_pb2.EvalArgs(num_steps=5000))\n",
        "context.run(trainer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6Cql1G35StJp"
      },
      "source": [
        "#### Analyze Training with TensorBoard\n",
        "Optionally, we can connect TensorBoard to the Trainer to analyze our model's training curves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bXe62WE0S0Ek",
        "colab": {}
      },
      "source": [
        "%%skip_for_export\n",
        "\n",
        "# Get the URI of the output artifact representing the training logs, which is a directory\n",
        "model_dir = trainer.outputs['model'].get()[0].uri\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {model_dir}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FmPftrv0lEQy"
      },
      "source": [
        "### Evaluator\n",
        "The `Evaluator` component computes model performance metrics over the evaluation set. It uses the [TensorFlow Model Analysis](https://www.tensorflow.org/tfx/model_analysis/get_started) library. The `Evaluator` can also optionally validate that a newly trained model is better than the previous model. This is useful in a production pipeline setting where you may automatically train and validate a model every day. In this notebook, we only train one model, so the `Evaluator` automatically will label the model as \"good\". \n",
        "\n",
        "`Evaluator` will take as input the data from `ExampleGen`, the trained model from `Trainer`, and slicing configuration. The slicing configuration allows you to slice your metrics on feature values (e.g. how does your model perform on taxi trips that start at 8am versus 8pm?). See an example of this configuration below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fVhfzzh9PDEx",
        "colab": {}
      },
      "source": [
        "eval_config = tfma.EvalConfig(\n",
        "    model_specs=[\n",
        "        # Using signature 'eval' implies the use of an EvalSavedModel. To use\n",
        "        # a serving model remove the signature to defaults to 'serving_default'\n",
        "        # and add a label_key.\n",
        "        tfma.ModelSpec(signature_name='eval')\n",
        "    ],\n",
        "    metrics_specs=[\n",
        "        tfma.MetricsSpec(\n",
        "            # The metrics added here are in addition to those saved with the\n",
        "            # model (assuming either a keras model or EvalSavedModel is used).\n",
        "            # Any metrics added into the saved model (for example using\n",
        "            # model.compile(..., metrics=[...]), etc) will be computed\n",
        "            # automatically.\n",
        "            metrics=[\n",
        "                tfma.MetricConfig(class_name='ExampleCount')\n",
        "            ],\n",
        "            # To add validation thresholds for metrics saved with the model,\n",
        "            # add them keyed by metric name to the thresholds map.\n",
        "            thresholds = {\n",
        "                'accuracy': tfma.MetricThreshold(\n",
        "                    value_threshold=tfma.GenericValueThreshold(\n",
        "                        lower_bound={'value': 0.5}),\n",
        "                    change_threshold=tfma.GenericChangeThreshold(\n",
        "                       direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "                       absolute={'value': -1e-10}))\n",
        "            }\n",
        "        )\n",
        "    ],\n",
        "    slicing_specs=[\n",
        "        # An empty slice spec means the overall slice, i.e. the whole dataset.\n",
        "        tfma.SlicingSpec(),\n",
        "        # Data can be sliced along a feature column. In this case, data is\n",
        "        # sliced along feature column trip_start_hour.\n",
        "        tfma.SlicingSpec(feature_keys=['trip_start_hour'])\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9mBdKH1F8JuT"
      },
      "source": [
        "Next, we give this configuration to `Evaluator` and run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zjcx8g6mihSt",
        "colab": {}
      },
      "source": [
        "# Use TFMA to compute a evaluation statistics over features of a model and\n",
        "# validate them against a baseline.\n",
        "\n",
        "# The model resolver is only required if performing model validation in addition\n",
        "# to evaluation. In this case we validate against the latest blessed model. If\n",
        "# no model has been blessed before (as in this case) the evaluator will make our\n",
        "# candidate the first blessed model.\n",
        "model_resolver = ResolverNode(\n",
        "      instance_name='latest_blessed_model_resolver',\n",
        "      resolver_class=latest_blessed_model_resolver.LatestBlessedModelResolver,\n",
        "      model=Channel(type=Model),\n",
        "      model_blessing=Channel(type=ModelBlessing))\n",
        "context.run(model_resolver)\n",
        "\n",
        "evaluator = Evaluator(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    model=trainer.outputs['model'],\n",
        "    #baseline_model=model_resolver.outputs['model'],\n",
        "    # Change threshold will be ignored if there is no baseline (first run).\n",
        "    eval_config=eval_config)\n",
        "context.run(evaluator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y5TMskWe9LL0"
      },
      "source": [
        "After `Evaluator` finishes running, we can show the default visualization of global metrics on the entire evaluation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U729j5X5QQUQ",
        "colab": {}
      },
      "source": [
        "%%skip_for_export\n",
        "\n",
        "context.show(evaluator.outputs['evaluation'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t-tI4p6m-OAn"
      },
      "source": [
        "To see the visualization for sliced evaluation metrics, we can directly call the TensorFlow Model Analysis library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pyis6iy0HLdi",
        "colab": {}
      },
      "source": [
        "%%skip_for_export\n",
        "\n",
        "import tensorflow_model_analysis as tfma\n",
        "\n",
        "# Get the TFMA output result path and load the result.\n",
        "PATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\n",
        "tfma_result = tfma.load_eval_result(PATH_TO_RESULT)\n",
        "\n",
        "# Show data sliced along feature column trip_start_hour.\n",
        "tfma.view.render_slicing_metrics(\n",
        "    tfma_result, slicing_column='trip_start_hour')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7uvYrUf2-r_6"
      },
      "source": [
        "This visualization shows the same metrics, but computed at every feature value of `trip_start_hour` instead of on the entire evaluation set.\n",
        "\n",
        "TensorFlow Model Analysis supports many other visualizations, such as Fairness Indicators and plotting a time series of model performance. To learn more, see [the tutorial](https://www.tensorflow.org/tfx/tutorials/model_analysis/tfma_basic).\n",
        "\n",
        "Since we added thresholds to our config, validation output is also available. The precence of a `blessing` artifact indicates that our model passed validation. Since this is the first validation being performed the candidate is automatically blessed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FXk1MA7sijCr",
        "colab": {}
      },
      "source": [
        "%%skip_for_export\n",
        "\n",
        "blessing_uri = evaluator.outputs.blessing.get()[0].uri\n",
        "!ls -l {blessing_uri}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "76Mil-7FlF_y"
      },
      "source": [
        "Now can also verify the success by loading the validation result record:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k4GghePOTJxL",
        "colab": {}
      },
      "source": [
        "%%skip_for_export\n",
        "\n",
        "PATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\n",
        "print(tfma.load_validation_result(PATH_TO_RESULT))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T8DYekCZlHfj"
      },
      "source": [
        "### Pusher\n",
        "The `Pusher` component is usually at the end of a TFX pipeline. It checks whether a model has passed validation, and if so, exports the model to `_serving_model_dir`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r45nQ69eikc9",
        "colab": {}
      },
      "source": [
        "pusher = Pusher(\n",
        "    model=trainer.outputs['model'],\n",
        "    model_blessing=evaluator.outputs['blessing'],\n",
        "    push_destination=pusher_pb2.PushDestination(\n",
        "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "            base_directory=_serving_model_dir)))\n",
        "context.run(pusher)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ctUErBYoTO9I"
      },
      "source": [
        "Let's examine the output artifacts of `Pusher`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pRkWo-MzTSss",
        "colab": {}
      },
      "source": [
        "%%skip_for_export\n",
        "\n",
        "pusher.outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "peH2PPS3VgkL"
      },
      "source": [
        "In particular, the Pusher will export your model in the SavedModel format, which looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4zyIqWl9TSdG",
        "colab": {}
      },
      "source": [
        "%%skip_for_export\n",
        "\n",
        "push_uri = pusher.outputs.model_push.get()[0].uri\n",
        "latest_version = max(os.listdir(push_uri))\n",
        "latest_version_path = os.path.join(push_uri, latest_version)\n",
        "model = tf.saved_model.load(latest_version_path)\n",
        "\n",
        "for item in model.signatures.items():\n",
        "  pp.pprint(item)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3-YPNUuHANtj"
      },
      "source": [
        "We're finished our tour of built-in TFX components!\n",
        "\n",
        "After you're happy with experimenting with TFX components and code in this notebook, you may want to export it as a pipeline to be orchestrated with Apache Airflow or Apache Beam. See the final section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qGNDOG1o1Tht"
      },
      "source": [
        "## Export to pipeline\n",
        "\n",
        "To export the contents of this notebook as a pipeline to be orchestrated with Airflow or Beam, follow the instructions below.\n",
        "\n",
        "If you're using Colab, make sure to **save this notebook to Google Drive** (`File` → `Save a Copy in Drive`) before exporting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DDbff6EdQ0iJ"
      },
      "source": [
        "### 1. Mount Google Drive (Colab-only)\n",
        "\n",
        "If you're using Colab, this notebook needs to mount your Google Drive to be able to access its own `.ipynb` file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "CH8yu7Un1Thu",
        "colab": {}
      },
      "source": [
        "%%skip_for_export\n",
        "\n",
        "#@markdown Run this cell and enter the authorization code to mount Google Drive.\n",
        "\n",
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  # Colab.\n",
        "  from google.colab import drive\n",
        "\n",
        "  drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iFrJ8nOIRIJ0"
      },
      "source": [
        "### 2. Select an orchestrator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "CO7erulbvUNi",
        "colab": {}
      },
      "source": [
        "_runner_type = 'beam' #@param [\"beam\", \"airflow\"]\n",
        "_pipeline_name = 'chicago_taxi_%s' % _runner_type"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d64gdS2u1Thw"
      },
      "source": [
        "### 3. Set up paths for the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X_dZL1lS1Thx",
        "colab": {}
      },
      "source": [
        "# For Colab notebooks only.\n",
        "# TODO(USER): Fill out the path to this notebook.\n",
        "_notebook_filepath = (\n",
        "    '/content/drive/My Drive/Colab Notebooks/taxi_pipeline_interactive.ipynb')\n",
        "\n",
        "# For Jupyter notebooks only.\n",
        "# _notebook_filepath = os.path.join(os.getcwd(),\n",
        "#                                   'taxi_pipeline_interactive.ipynb')\n",
        "\n",
        "# TODO(USER): Fill out the paths for the exported pipeline.\n",
        "_pipeline_name = 'taxi_pipeline'\n",
        "_tfx_root = os.path.join(os.environ['HOME'], 'tfx')\n",
        "_taxi_root = os.path.join(os.environ['HOME'], 'taxi')\n",
        "_serving_model_dir = os.path.join(_taxi_root, 'serving_model')\n",
        "_data_root = os.path.join(_taxi_root, 'data', 'simple')\n",
        "_pipeline_root = os.path.join(_tfx_root, 'pipelines', _pipeline_name)\n",
        "_metadata_path = os.path.join(_tfx_root, 'metadata', _pipeline_name,\n",
        "                              'metadata.db')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "stNBJAIPvUNq"
      },
      "source": [
        "### 4. Choose components to include in the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DNc0Iks2vUNq",
        "colab": {}
      },
      "source": [
        "# TODO(USER): Specify components to be included in the exported pipeline.\n",
        "components = [\n",
        "    example_gen, statistics_gen, schema_gen, example_validator, transform,\n",
        "    trainer, model_resolver, evaluator, pusher\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6nATTNYZ1Thy"
      },
      "source": [
        "### 5. Generate pipeline files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "SsfNFi6iHMSp",
        "colab": {}
      },
      "source": [
        "%%skip_for_export\n",
        "\n",
        "#@markdown Run this cell to generate the pipeline files.\n",
        "\n",
        "if get_ipython().magics_manager.auto_magic:\n",
        "  print('Warning: %automagic is ON. Line magics specified without the % prefix '\n",
        "        'will not be scrubbed during export to pipeline.')\n",
        "\n",
        "_pipeline_export_filepath = 'export_%s.py' % _pipeline_name\n",
        "context.export_to_pipeline(notebook_filepath=_notebook_filepath,\n",
        "                           export_filepath=_pipeline_export_filepath,\n",
        "                           runner_type=_runner_type)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qL4RQQwSSt0y"
      },
      "source": [
        "### 6. Download pipeline files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "FeRJyHly1Th3",
        "colab": {}
      },
      "source": [
        "%%skip_for_export\n",
        "\n",
        "#@markdown Run this cell to download the pipeline files as a `.zip`.\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import files\n",
        "  import zipfile\n",
        "\n",
        "  zip_export_path = os.path.join(\n",
        "      tempfile.mkdtemp(), 'export.zip')\n",
        "  with zipfile.ZipFile(zip_export_path, mode='w') as export_zip:\n",
        "    export_zip.write(_pipeline_export_filepath)\n",
        "    export_zip.write(_taxi_constants_module_file)\n",
        "    export_zip.write(_taxi_transform_module_file)\n",
        "    export_zip.write(_taxi_trainer_module_file)\n",
        "\n",
        "  files.download(zip_export_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Po3wc1dMTJHw"
      },
      "source": [
        "To learn how to run the orchestrated pipeline with Apache Airflow, please refer to the [TFX Orchestration Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/airflow_workshop)."
      ]
    }
  ]
}